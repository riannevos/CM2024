{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering - voorbeelden\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k-Means - voorbeeld 1 ijswinkels\n",
    "\n",
    "In dit voorbeeld gaan we op zoek naar logische clusters binnen een keten van ijswinkels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# We hebben voor acht winkels de verkochte hoeveelheid ijsjes in de smaken chocolade en vanille\n",
    "X = [[12, 6],\n",
    "     [15,16],\n",
    "     [18,17],\n",
    "     [10, 8],\n",
    "     [ 8, 7],\n",
    "     [ 9, 6],\n",
    "     [12, 9],\n",
    "     [20,18]]\n",
    "\n",
    "# doe de clustering\n",
    "iceMeans = KMeans(n_clusters=2, random_state=0).fit(X)\n",
    "\n",
    "# druk de middelpunten af\n",
    "print(iceMeans.cluster_centers_)\n",
    "\n",
    "# deel twee nieuwe winkels in:\n",
    "print(iceMeans.predict([[11, 15], [20, 4]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dit zijn de clustermiddelpunten\n",
    "print(iceMeans.cluster_centers_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In dit dataframe gaan we de initiële gegevens opslaan en later ook de gevonden clusters toevoegen\n",
    "ijsdf = pd.DataFrame(X)\n",
    "ijsdf.columns = ['Chocolate', 'Vanille']\n",
    "ijsdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# De gevonden clusters voegen we hier toe \n",
    "ijsdf['Groep'] = iceMeans.labels_\n",
    "ijsdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We gaan de clusters nu visualiseren, dat is handig om te kunnen bepalen wat het verschil is tussen beide clusters\n",
    "# Je zou misschien verwachten dat er winkels zijn die vooral veel chocolade verkopen en andere die juist veel vanille verkopen\n",
    "plt.scatter(ijsdf['Chocolate'], ijsdf['Vanille'], c=ijsdf['Groep'], cmap='plasma')\n",
    "plt.xlim(0,25)\n",
    "plt.ylim(0,25)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Het blijkt dat er drie winkels zijn die meer verkopen dan gemiddeld en vijf winkels die minder verkopen. Er is dus geen chocoladecluster en vanillecluster!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## kMeans versus Gaussian Mixture Models - voorbeeld\n",
    "\n",
    "In dit notebook laat ik zien hoe kMeans en GMM werken, wat de overeenkomsten en wat de verschillen zijn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.spatial import Voronoi, voronoi_plot_2d\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code van https://jakevdp.github.io/PythonDataScienceHandbook/05.12-gaussian-mixtures.html\n",
    "# Generate some data\n",
    "from sklearn.datasets import make_blobs\n",
    "X, y_true = make_blobs(n_samples=400, centers=3,\n",
    "                       cluster_std=0.60, random_state=0)\n",
    "X = X[:, ::-1] # flip axes for better plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatterplot van de punten\n",
    "# Je ziet 3 'wolkjes' van data\n",
    "plt.scatter(X[:,0],X[:,1],marker='o')\n",
    "plt.gcf().set_size_inches((10, 10))  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### kMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Maak een kMeans model aan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_kMeans = KMeans()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Kies de dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_kMeans = X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Kies de hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We spelen vals, want we weten dat er 3 clusters zijn. Maar speel gerust met deze waarde en kijk wat het effect is!\n",
    "number_clusters = 3\n",
    "model_kMeans.set_params(n_clusters = number_clusters, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Train het model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_kMeans.fit(X_kMeans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Toon de resultaten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Het resultaat is dat elk van de punten uit X_kMeans is toegevoegd aan een cluster\n",
    "prediction_kMeans = model_kMeans.fit_predict(X_kMeans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In deze scatterplot heb ik de middelpunten (kMeans.cluster_centers_) toegevoegd, zie de 'x'\n",
    "plt.scatter(X_kMeans[:,0],X_kMeans[:,1],marker='o', c=prediction_kMeans)\n",
    "plt.scatter(model_kMeans.cluster_centers_[:,0],model_kMeans.cluster_centers_[:,1],marker='x', s=40)\n",
    "plt.gcf().set_size_inches((10, 10))  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualiseren met cirkels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code van https://jakevdp.github.io/PythonDataScienceHandbook/05.12-gaussian-mixtures.html\n",
    "# Deze code helpt om cirkels te plotten \n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "def plot_kmeans(kMeans, X, labels, n_clusters=number_clusters, rseed=0, ax=None):\n",
    "    \n",
    "    # plot the input data\n",
    "    ax = ax or plt.gca()\n",
    "    plt.gcf().set_size_inches((10, 10))\n",
    "    plt.scatter(kMeans.cluster_centers_[:,0],kMeans.cluster_centers_[:,1],marker='x', s=40)\n",
    "    ax.axis('equal')\n",
    "    ax.scatter(X[:, 0], X[:, 1], c=labels, s=40, cmap='viridis', zorder=2)\n",
    "    # plot the representation of the KMeans model\n",
    "    centers = kMeans.cluster_centers_\n",
    "    radii = [cdist(X[labels == i], [center]).max()\n",
    "             for i, center in enumerate(centers)]\n",
    "    for c, r in zip(centers, radii):\n",
    "        ax.add_patch(plt.Circle(c, r, fc='#CCCCCC', lw=3, alpha=0.5, zorder=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Een veelgebruikte manier om gevonden clusters te visualiseren is met behulp van cirkels. \n",
    "# In onderstaande scatterplot zijn cirkels getoond voor elk cluster\n",
    "plot_kmeans(model_kMeans, X_kMeans, prediction_kMeans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Het gebruik van cirkels is handig om aan te tonen welke punten in een bepaald cluster vallen maar kan wel verwarrend zijn. \n",
    "Je mag er sowieso **niet** vanuit gaan dat alle punten in een cirkel noodzakelijkerwijs bij een bepaald cluster horen. Verander het aantal clusters bijvoorbeeld naar 4.\n",
    "\n",
    "Voor elk **nieuw** punt moet je dus alleen naar de afstand tot de middelpunten kijken, niet naar deze cirkels. De cirkels zijn alleen bedoeld om te helpen met de logica achter de gevonden clusters te bepalen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualiseren met behulp van een Voronoi-diagram\n",
    "Het gebruik maken van een Voronoi diagram kan nuttig zijn. De clusters worden daarin gescheiden door 'boundaries'. Een punt zal niet in meerdere clusters kunnen vallen hierdoor.\n",
    "\n",
    "Let ook hier wel op: de boundaries zijn net als de cirkels verwarrend. Neem een willekeurig nieuw punt met coördinaten (0, 1000), dat valt binnen het vlak van het groene cluster maar ligt enorm ver van het clustermiddelpunt af. Het is de vraag of dit punt dan ook bij het groene cluster zou moeten horen of niet..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Voronoi diagram plotten i.c.m. scatter plot\n",
    "vor = Voronoi(model_kMeans.cluster_centers_)\n",
    "fig = voronoi_plot_2d(vor)\n",
    "plt.gcf().set_size_inches((10, 10))\n",
    "\n",
    "ax = fig.add_subplot()\n",
    "ax.axis('equal')\n",
    "ax.scatter(X_kMeans[:, 0], X_kMeans[:, 1], c=prediction_kMeans, s=40, cmap='viridis', zorder=2)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualiseren met behulp cirkels én een Voronoi-diagram\n",
    "Je kunt de cirkels en het Voronoi-diagram natuurlijk ook combineren. Dat geeft net even iets meer informatie. Het punt (0, 1000) valt wel in het groene vlak maar zou heel ver buiten cirkel liggen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dit is een uitbreiding van de functie plot_kmeans om de cirkels samen met de Voronoi te kunnen plotten\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "def plot_kmeans_voronoi(kMeans, X, labels, n_clusters=number_clusters, rseed=0, ax=None):\n",
    "    vor = Voronoi(model_kMeans.cluster_centers_)\n",
    "    fig = voronoi_plot_2d(vor)\n",
    "    plt.gcf().set_size_inches((10, 10))\n",
    "    ax = fig.add_subplot()    \n",
    "    ax.axis('equal')\n",
    "\n",
    "    # plot the input data\n",
    "    plt.scatter(model_kMeans.cluster_centers_[:,0],model_kMeans.cluster_centers_[:,1],marker='x', s=40)\n",
    "    ax.scatter(X[:, 0], X[:, 1], c=labels, s=40, cmap='viridis', zorder=2)\n",
    "\n",
    "    # plot the representation of the KMeans model\n",
    "    centers = model_kMeans.cluster_centers_\n",
    "    radii = [cdist(X[labels == i], [center]).max()\n",
    "             for i, center in enumerate(centers)]\n",
    "    for c, r in zip(centers, radii):\n",
    "        ax.add_patch(plt.Circle(c, r, fc='#CCCCCC', lw=3, alpha=0.5, zorder=1))\n",
    "        \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_kmeans_voronoi(model_kMeans, X_kMeans, prediction_kMeans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Je ziet in bovenstaande plaat dat de boundary tussen het paarse en gele cluster 'overschreden' wordt door beide cirkels.\n",
    "Ik hoop hiermee aangetoond te hebben wat het probleem is met het gebruik van cirkels voor het visualiseren van de clusters en hoe de combinatie van de cirkels met het Voronoi-diagram goed kan werken."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nadelen van kMeans\n",
    "Voor clusters van kMeans wordt een afstandsberekening uitgevoerd waarbij alle dimensies even zwaar mee tellen. Daarom worden cirkels gebruikt om de clusters te visualiseren: de punten op de rand van de cirkel zitten allemaal even ver van het middelpunt. \n",
    "In onderstaand voorbeeld wordt aangetoond dat dit voor sommige datasets geen handige aanpak is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code komt van https://jakevdp.github.io/PythonDataScienceHandbook/05.12-gaussian-mixtures.html\n",
    "# Deze dataset bevat datapunten die niet in een 'ronde' wolk liggen, maar in meer ellipsachtige vorm\n",
    "# We voeren stappen 1. t/m 5. nu in één keer uit\n",
    "\n",
    "model_kMeans2 = KMeans(n_clusters=3, random_state=0)\n",
    "rng = np.random.RandomState(13)\n",
    "X_stretched = np.dot(X_kMeans, rng.randn(2, 2))\n",
    "prediction_kMeans2 = model_kMeans2.fit_predict(X_stretched)\n",
    "plot_kmeans(model_kMeans2, X_stretched, prediction_kMeans2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Je ziet nu dat er enorm veel overlap is tussen het gele en groene cluster. Met het blote oog zie je dat er ook geen sprake is van ronde clusters, maar eerder langwerpige clusters. De clustering lijkt dus niet logisch en dat komt door de beperkingen van kMeans. Gelukkig hebben we ook Gaussian Mixture Models om clusters te zoeken."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Mixture Models\n",
    "Gaussian Mixture Models kun je omschrijven als een verbeterde versie van kMeans. GMM's kunnen met verschillende clustervormen overweg en werken daarnaast met probabilities. Een punt kan met kans 1.0 in een bepaald cluster wanneer het heel dicht bij een clustermiddelpunt zit. Maar voor sommige punten die tussen twee clustermiddelpunten zitten is het moeilijk om te zeggen cluster A of B. Dan wordt met een kans aangegeven hoe groot de kans is dat het punt bij cluster A of B hoort. Dat is relaxed want zoals we weten kan en hoeft een model niet altijd dezelfde zekerheid bieden bij het maken van een voorspelling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Maak een GMM model aan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_gmm = GaussianMixture()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Kies de dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_gmm = X_stretched"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Kies de hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_clusters = 3\n",
    "model_gmm.set_params(n_components = number_clusters, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Train het model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_gmm.fit(X_gmm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Toon de klassen van de gegeven waarden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bepaal eerst de clusters van elk punt\n",
    "prediction_gmm = model_gmm.predict(X_gmm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In de scatterplot heb ik de middelpunten toegevoegd\n",
    "plt.scatter(X_gmm[:,0],X_gmm[:,1],marker='o', c=prediction_gmm)\n",
    "plt.scatter(model_gmm.means_[:,0],model_gmm.means_[:,1],marker='x', s=40)\n",
    "plt.gcf().set_size_inches((10, 10))  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code komt van https://jakevdp.github.io/PythonDataScienceHandbook/05.12-gaussian-mixtures.html\n",
    "# Deze code helpt om ellipsen te plotten\n",
    "from matplotlib.patches import Ellipse\n",
    "\n",
    "def draw_ellipse(position, covariance, ax=None, **kwargs):\n",
    "    \"\"\"Draw an ellipse with a given position and covariance\"\"\"\n",
    "    ax = ax or plt.gca()\n",
    "    plt.gcf().set_size_inches((10, 10))  \n",
    "\n",
    "    # Convert covariance to principal axes\n",
    "    if covariance.shape == (2, 2):\n",
    "        U, s, Vt = np.linalg.svd(covariance)\n",
    "        angle = np.degrees(np.arctan2(U[1, 0], U[0, 0]))\n",
    "        width, height = 2 * np.sqrt(s)\n",
    "    else:\n",
    "        angle = 0\n",
    "        width, height = 2 * np.sqrt(covariance)\n",
    "    \n",
    "    # Draw the Ellipse\n",
    "    for nsig in range(1, 4):\n",
    "        ax.add_patch(Ellipse(position, nsig * width, nsig * height,\n",
    "                             angle, **kwargs))\n",
    "        \n",
    "def plot_gmm(gmm, X, labels, label=True, ax=None):\n",
    "    ax = ax or plt.gca()\n",
    "    if label:\n",
    "        ax.scatter(X[:, 0], X[:, 1], c=labels, s=20, cmap='viridis', zorder=2)\n",
    "    else:\n",
    "        ax.scatter(X[:, 0], X[:, 1], s=20, zorder=2)\n",
    "    ax.axis('equal')\n",
    "    \n",
    "    w_factor = 0.2 / gmm.weights_.max()\n",
    "    for pos, covar, w in zip(gmm.means_, gmm.covariances_, gmm.weights_):\n",
    "        draw_ellipse(pos, covar, alpha=w * w_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hieronder wordt gevisualiseerd hoe de clusters eruit zien\n",
    "plot_gmm(model_gmm, X_gmm, prediction_gmm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Je ziet dat de clusters geen ronde vorm hebben maar een ellipsachtige vorm.\n",
    "Je ziet ook verschillende kleuren: hoe lichter de kleur blauw, hoe verder een punt van het clustermiddelpunt is en hoe kleiner de kans dat een punt tot een cluster behoort."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Het model in de praktijk gebruiken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We gaan voor vier punten bepalen tot welk cluster ze behoren:\n",
    "# 1. het eerst gevonden clustermiddelpunt, \n",
    "# 2. een punt tussen het gele en paarse cluster (-2.8, 2.8),\n",
    "# 3. een punt dat tussen het gele, paarse én groene cluster valt (-1.7, 2),\n",
    "# 4. een punt dat enorm uit de richting zit (0, 6).\n",
    "\n",
    "X_extra_points = np.array([model_gmm.means_[1],[-2.8,2.8],[-1.7,2],[0,6]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hieronder zie je de vier nieuwe punten in de scatter plot met een sterretje\n",
    "plt.scatter(X_gmm[:,0],X_gmm[:,1],marker='o', c=prediction_gmm)\n",
    "plt.scatter(model_gmm.means_[:,0],model_gmm.means_[:,1],marker='x', s=200)\n",
    "plt.scatter(X_extra_points[:,0], X_extra_points[:,1], marker=(5, 1), s=200, color='red')\n",
    "plt.gcf().set_size_inches((10, 10))  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We gaan hieronder bepalen tot welke clusters deze punten behoren en aanvullende scores op een rijtje zetten\n",
    "\n",
    "# Ten eerste bepalen we voor elk punt bij welk cluster deze hoort\n",
    "clusters = np.around(model_gmm.predict(X_extra_points), decimals=1)\n",
    "\n",
    "# Ten tweede bepalen we voor elk punt wat de waarschijnlijkheid is van die clustertoekenning\n",
    "clusters_proba = np.around(model_gmm.predict_proba(X_extra_points), decimals=1)\n",
    "\n",
    "# Tenslotte bepalen we voor elk sample de score van de clustertoekenning\n",
    "clusters_score_samples = np.around(model_gmm.score_samples(X_extra_points), decimals=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deze scores gaan we visualiseren\n",
    "from matplotlib.colors import ListedColormap, LinearSegmentedColormap\n",
    "from matplotlib import cm\n",
    "viridis = cm.get_cmap('viridis', 3)\n",
    "\n",
    "fig, axs = plt.subplots(2, 2)\n",
    "plt.gcf().set_size_inches((10, 10)) \n",
    "fig\n",
    "axs[0, 0].set_title('Punt: ' + str(X_extra_points[0]))\n",
    "axs[0, 0].set_xlabel('Cluster: ' + str(clusters[0]) + ' met score ' + str(clusters_score_samples[0]))\n",
    "axs[0, 0].pie(clusters_proba[0], autopct='%1.1f%%', shadow=True, colors=viridis(range(4)))\n",
    "\n",
    "axs[0, 1].set_title('Punt: ' + str(X_extra_points[1]))\n",
    "axs[0, 1].set_xlabel('Cluster: ' + str(clusters[1]) + ' met score ' + str(clusters_score_samples[1]))\n",
    "axs[0, 1].pie(clusters_proba[1], autopct='%1.1f%%', shadow=True, colors=viridis(range(4)))\n",
    "\n",
    "axs[1, 0].set_title('Punt: ' + str(X_extra_points[2]))\n",
    "axs[1, 0].set_xlabel('Cluster: ' + str(clusters[2]) + ' met score ' + str(clusters_score_samples[2]))\n",
    "axs[1, 0].pie(clusters_proba[2], autopct='%1.1f%%', shadow=True, colors=viridis(range(4)))\n",
    "\n",
    "axs[1, 1].set_title('Punt: ' + str(X_extra_points[3]))\n",
    "axs[1, 1].set_xlabel('Cluster: ' + str(clusters[3]) + ' met score ' + str(clusters_score_samples[3]))\n",
    "axs[1, 1].pie(clusters_proba[3], autopct='%1.1f%%', shadow=True, colors=viridis(range(4)))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Je ziet een aantal verschillen:\n",
    "1. Punt 1 en 4 zijn allebei met een waarschijnlijkheid van 1 toegekend aan hun cluster.\n",
    "    - Voor punt 1 is dat logisch, want dat is een clustermiddelpunt\n",
    "    - Punt 4 is de *uitschieter*, die zit vooral dichtbij cluster 1, maar nog steeds erg ver weg\n",
    "    - Je ziet daarom dat de score voor punt 4 veel lager is (-200 ongeveer) omdat deze veel verder van het clustermiddelpunt ligt dan punt 1 (score is -0.7).\n",
    "2. Punten 2 en 3 zijn niet voor 100% toegekend aan één cluster maar aan respectievelijk 2 en 3 clusters. De hoogste waarschijnlijkheden zijn ook nog eens verschillend: 80% versus 50%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kwaliteit clustering bepalen\n",
    "De kwaliteit van de gevonden clusters zul je sowieso handmatig moeten beoordelen: zijn de gevonden clusters 'logisch'? Kun je er iets mee in de praktijk? Dat zijn de belangrijkste manieren om te bepalen of kwaliteit van de clusters goed is.\n",
    "\n",
    "Daarnaast kun je de kwaliteit ook in een cijfer uitdrukken. De score van een clustering wordt gebaseerd op de afstanden van alle punten tot het middelpunt van hun cluster. Hoe dichter die score bij 0 komt, hoe dichter de punten bij hun clustermiddelpunt zitten en hoe dichter de punten bij elkaar zitten. Een goede score is positief, maar een té goede score is niet positief: dan kan er sprake van overfitting zijn. \n",
    "\n",
    "De allerlaagste bereik je door het aantal clusters gelijk te stellen aan het aantal punten: elk punt heeft dan z'n eigen cluster waarin dat punt het clustermiddelpunt 0 is. Maar dat is natuurlijk een clustering die zeer matig is...\n",
    "\n",
    "Hieronder wordt getoond hoe je het ideale aantal clusters kunt vinden. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In deze code wordt een kMeans model gemaakt voor 1, 2, ... n aantal clusters. De scores worden daarna geplot.\n",
    "# We maken hier weer gebruik van de dataset waarmee we begonnen zijn\n",
    "data = []\n",
    "\n",
    "max_n = 20\n",
    "\n",
    "for i in range(1, max_n):\n",
    "    model_kMeans = KMeans(n_clusters=i, random_state=0)\n",
    "    prediction_kMeans = model_kMeans.fit_predict(X_kMeans)\n",
    "    data.append([i, model_kMeans.score(X_kMeans)])\n",
    "\n",
    "df = pd.DataFrame(data, columns=['k', 'Score'])\n",
    "\n",
    "fig = plt.figure(figsize=(5,5), dpi=150)\n",
    "\n",
    "# object axes: de grafiekbasis\n",
    "ax = plt.axes()\n",
    "\n",
    "ax.set(xlim=(0,max_n),\n",
    "       xlabel='k',\n",
    "       ylabel='Score',\n",
    "       title='kMeans: score vs k')\n",
    "\n",
    "ax.xaxis.set_major_locator(plt.MaxNLocator(20))\n",
    "ax.ticklabel_format(useOffset=False)\n",
    "ax.plot(df['k'], df['Score'], '-o')\n",
    "\n",
    "ax.legend(['k'])\n",
    "ax.annotate('Knik', xy=(4,-350),xytext=(10,-1200), arrowprops=dict(facecolor='black',shrink=0.05))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In bovenstaande diagram zie je de foutscore van ongeveer -3.800 naar 0 convergeren. Je ziet dat er van k = 1 naar k = 2 een hele grote verbetering plaats vindt. Van k = 2 naar k = 3 en naar k = 4 vinden ook nog substantiële verbeteringen plaats. Daarna zie je dat het afzwakt.\n",
    "\n",
    "Het idee is dat op de 'knik' (daarvoor zijn substantiële verbeteringen, daarna zwakt het af) de ideale waarde voor k zit. In dit voorbeeld zijn dat de 4 clusters waarmee we begonnen zijn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In deze code wordt een GMM model gemaakt voor 1, 2, ... n aantal clusters. De scores worden daarna geplot.\n",
    "# We maken hier weer gebruik van de dataset waarmee we begonnen zijn\n",
    "data = []\n",
    "\n",
    "max_n = 20\n",
    "\n",
    "for i in range(1, max_n):\n",
    "    model_gmm = GaussianMixture(n_components=i, random_state=0)\n",
    "    prediction_gmm = model_gmm.fit(X_gmm)\n",
    "    data.append([i, model_gmm.score(X_gmm)])\n",
    "\n",
    "df = pd.DataFrame(data, columns=['n', 'Score'])\n",
    "\n",
    "fig = plt.figure(figsize=(5,5), dpi=150)\n",
    "\n",
    "# object axes: de grafiekbasis\n",
    "ax = plt.axes()\n",
    "\n",
    "ax.set(xlim=(0,max_n),\n",
    "       xlabel='n',\n",
    "       ylabel='Score',\n",
    "       title='GMM: score vs n')\n",
    "\n",
    "ax.xaxis.set_major_locator(plt.MaxNLocator(20))\n",
    "ax.ticklabel_format(useOffset=False)\n",
    "ax.plot(df['n'], df['Score'], '-o')\n",
    "\n",
    "ax.legend(['n'])\n",
    "ax.annotate('Knik', xy=(4,-1.7),xytext=(7,-1.8), arrowprops=dict(facecolor='black',shrink=0.05))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De knik is een *indicator* dat dit het optimale aantal clusters, maar dat is puur geredeneerd vanuit *overfitting*, niet vanuit wat logisch is. Bepaal dus ook altijd handmatig of de gevonden clusters logisch zijn of niet: kun je de gevonden groepen dus verklaren?"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
